{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Habeebhassan/Datafest_Fraud_Detection/blob/main/Datafest_Fraud_Detection_for_Online_Payment_Platform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax_2fsMwePKe"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "Online transactions have become increasingly popular, and this trend is expected to continue in the future, according to various surveys and research. However, this growth has also led to an increase in fraudulent transactions. Despite the implementation of various security systems, a significant amount of money is still being lost due to fraudulent transactions. Online fraud transactions occur when a person uses someone else’s credit card for personal reasons without the owner or the card-issuing authorities being aware of it. This project aims to address this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z-gIr9ZeITr"
      },
      "source": [
        "**Project Scope**\n",
        "\n",
        "The Online Fraud Transaction Detection System is an extension of an existing system. The algorithms built using this system will go through the dataset and provide the appropriate output. In the long run, this system will be beneficial as it provides an efficient way to create a secure transaction system to analyze and detect fraudulent transactions. The Proposed algorithm algorithm used in this project is XGBOOST. Xgboost algorithm is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models. This accuracy can be increased further by providing a huge dataset for model training. The scope of this application is far-reaching, and it can be used to detect the features of fraud transactions in datasets that are applicable in various sectors such as banking, insurance, e-commerce, money transfer, bill payments, etc. This will help increase security."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxWGjyfHfNXs"
      },
      "source": [
        "**Work Flow**\n",
        "\n",
        "- Load DataSet\n",
        "- Data Preprocessing\n",
        "- Feature Selection/Feature Engineering\n",
        "- Classification\n",
        "- XGboost Model Training\n",
        "- Prediction\n",
        "- Evaluation\n",
        "\n",
        "After preprocessing and feature engineering is done, I'll divide the data into two sets: a training set and a separate test set that will be used exclusively at the final stage of model development. This ensures an unbiased assessment of the model's performance on entirely new data.\n",
        "\n",
        "Next, I'll go through the following steps to construct and assess the model:\n",
        "\n",
        "Hyperparameter Tuning with Cross Validation: Given the considerable number of hyperparameters in XGBoost, I'll employ Bayesian hyperparameter tuning. This approach is more efficient compared to grid or random search. I'll use (stratified) K-fold cross validation to identify the combination of hyperparameters that yields the highest cross-validated Conditional Recall score.\n",
        "\n",
        "Determining Thresholds: The refined classifier from step one can assign a probability score to any given example. To classify an example, a probability threshold must be chosen. This threshold separates positive (fraud) and negative examples. While the standard practice is to set the threshold at 0.5, I'll explore empirical thresholding to strike a balance between precision and recall. This may lead to a higher recall rate by selecting an appropriate classification threshold.\n",
        "\n",
        "Training and Testing: Using the entire training set, I'll train the model and then assess its performance on the test set using the discussed Conditional Recall metric.\n",
        "\n",
        "Additionally, I'll conclude by comparing the performance of this model with a few other algorithms. While the ideal approach would involve nested cross-validation for comparing different models (e.g., XGBoost vs. Logistic Regression), this is computationally demanding. Therefore, I'll report the performance of these models based on a single test set.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0qd0HAS4_2y"
      },
      "source": [
        "**Proposed Algorithm**\n",
        "\n",
        "The XGBoost algorithm, short for Extreme Gradient Boosting, is a powerful implementation of the gradient boosting method. Gradient boosting is an ensemble learning technique that sequentially trains simple models, often shallow decision trees, with a focus on areas of the data that haven't been well predicted so far. The final prediction of the model is a weighted combination of these weak learners. XGBoost has demonstrated remarkable effectiveness in various regression and classification tasks.\n",
        "\n",
        "For this task, I'll employ the XGBClassifier class from the xgboost package's Python API. This classifier utilizes the Extreme Gradient Boosting algorithm to optimize a specified loss function. In this case, I'll be using a logistic loss function, which is also the default choice in xgboost for two-class classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YxQLJm22342"
      },
      "source": [
        "**Bayesian Hyper-parameter Optimization**\n",
        "\n",
        "Bayesian hyper-parameter optimization commences by assuming a prior distribution for the model's parameters. In each iteration, it seeks to glean insights from previously assessed parameter values, updating this distribution to select values more likely to yield high scores in future trials. This stands in contrast to random and grid search methods, which explore the parameter space without leveraging past trials for guidance. As a result, Bayesian hyper-parameter tuning has demonstrated superior efficiency compared to both random search and grid search, particularly in scenarios where evaluating the objective function is resource-intensive and time-consuming, or when dealing with a high-dimensional parameter-space.\n",
        "\n",
        "In the visual representation, the grey dots represent results from random search, while the green dots depict outcomes from Bayesian optimization (utilizing the Tree Parzen Estimator or TPE). Each dot signifies the lowest validation set error achieved within a specific number of trials for the respective method. It's evident that Bayesian optimization outperforms random search, and it does so with fewer iterations.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuJJ3Pvx3bBg"
      },
      "source": [
        "**Optimizing Hyperparameters Using Hyperopt**\n",
        "\n",
        "For fine-tuning the model's hyperparameters, I'll employ the Hyperopt package, which employs Bayesian optimization technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugReQKr85dVp"
      },
      "source": [
        "**Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn-E5CYMjIOZ",
        "outputId": "4c7e2862-47e8-44a5-bd37-f3f14d1e18dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries\n",
            "Collecting pyforest\n",
            "  Downloading pyforest-1.1.0.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyforest\n",
            "  Building wheel for pyforest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyforest: filename=pyforest-1.1.0-py2.py3-none-any.whl size=14605 sha256=da21f800dec35ee6d895ffda32846bae47949a6f76e86872d66869aa220e42ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/7d/2c/5d2f5e62de376c386fd3bf5a8e5bd119ace6a9f48f49df6017\n",
            "Successfully built pyforest\n",
            "Installing collected packages: pyforest\n",
            "Successfully installed pyforest-1.1.0\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.2-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.2\n",
            "Installing necessary libraries\n",
            "Requirement already satisfied: pyforest in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n",
            "Mounting Google drive to load data\n",
            "Mounted at /content/gdrive\n",
            "lmporting libraries\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing necessary libraries\")\n",
        "!pip install pyforest\n",
        "!pip install category_encoders\n",
        "\n",
        "print(\"Installing necessary libraries\")\n",
        "!pip install pyforest\n",
        "!pip install category_encoders\n",
        "\n",
        "print(\"Mounting Google drive to load data\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "print(\"lmporting libraries\")\n",
        "from pyforest import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6GQB9_UhEWC"
      },
      "source": [
        "# Other Relevant libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4xduh2z_g9PG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# For Bayesian hyper-parameter optimization\n",
        "import hyperopt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer, precision_recall_curve, recall_score, precision_score\n",
        "from functools import partial\n",
        "# To supress a deprecation warning caused due to an issue between XGBoost and SciPy\n",
        "import warnings\n",
        "\n",
        "MIN_PRECISION = 0.05\n",
        "\n",
        "# The current version of XGBoost uses a conditional statement that\n",
        "# the current version SciPy (internally used by XGBoost) doesn't like.\n",
        "# This supresses SciPy's deprecation warning message\n",
        "warnings.filterwarnings('ignore', category = DeprecationWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUCXClYfRpn"
      },
      "source": [
        "Load Dataset from Gdrive to save resources on Local computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LfDmKc8ojJGr"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('gdrive/MyDrive/Fraud_Detection_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Fraudulent Flag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8j1szkDbx8X",
        "outputId": "00bb5b3f-2b8e-4f1d-ed72-eec3d3aa2768"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    3000094\n",
              "1    2999906\n",
              "Name: Fraudulent Flag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mug6RniD-N1P",
        "outputId": "faa4d289-bc12-4831-a5d9-a3b4d3070482"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Transaction ID', 'User ID', 'Transaction Amount',\n",
              "       'Transaction Date and Time', 'Merchant ID', 'Payment Method',\n",
              "       'Country Code', 'Transaction Type', 'Device Type', 'IP Address',\n",
              "       'Browser Type', 'Operating System', 'Merchant Category', 'User Age',\n",
              "       'User Occupation', 'User Income', 'User Gender', 'User Account Status',\n",
              "       'Transaction Status', 'Location Distance', 'Time Taken for Transaction',\n",
              "       'Transaction Time of Day', 'User's Transaction History',\n",
              "       'Merchant's Reputation Score', 'User's Device Location',\n",
              "       'Transaction Currency', 'Transaction Purpose', 'User's Credit Score',\n",
              "       'User's Email Domain', 'Merchant's Business Age',\n",
              "       'Transaction Authentication Method', 'Fraudulent Flag'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJBOqUqW-4mZ",
        "outputId": "2de56891-1eaa-4e2c-bcc0-e5070175c9dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transaction ID                         int64\n",
              "User ID                                int64\n",
              "Transaction Amount                   float64\n",
              "Transaction Date and Time             object\n",
              "Merchant ID                            int64\n",
              "Payment Method                        object\n",
              "Country Code                          object\n",
              "Transaction Type                      object\n",
              "Device Type                           object\n",
              "IP Address                            object\n",
              "Browser Type                          object\n",
              "Operating System                      object\n",
              "Merchant Category                     object\n",
              "User Age                               int64\n",
              "User Occupation                       object\n",
              "User Income                          float64\n",
              "User Gender                           object\n",
              "User Account Status                   object\n",
              "Transaction Status                    object\n",
              "Location Distance                    float64\n",
              "Time Taken for Transaction           float64\n",
              "Transaction Time of Day               object\n",
              "User's Transaction History             int64\n",
              "Merchant's Reputation Score          float64\n",
              "User's Device Location                object\n",
              "Transaction Currency                  object\n",
              "Transaction Purpose                   object\n",
              "User's Credit Score                    int64\n",
              "User's Email Domain                   object\n",
              "Merchant's Business Age                int64\n",
              "Transaction Authentication Method     object\n",
              "Fraudulent Flag                        int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91ILmjKfl8K"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YQwJ5SLT_mdx"
      },
      "outputs": [],
      "source": [
        "X = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U2I657JyACTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c355a6-896b-40d4-f595-a34e6e45fa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # # handling IP Address with high cardinality\n",
        "\n",
        "# # Calculate the frequency of each IP address\n",
        "# ip_address_counts = X['IP Address'].value_counts(normalize=True)\n",
        "\n",
        "# # Create a new column with the frequency of each IP address\n",
        "# X['IP Address Frequency'] = X['IP Address'].map(ip_address_counts)\n",
        "\n",
        "# # Replacing missing values (if any) with 0 (assuming you want to treat unseen IPs as less common)\n",
        "# X['IP Address Frequency'].fillna(0, inplace=True)\n",
        "\n",
        "# # Hashing function IP addresses\n",
        "ip_addresses = X['IP Address'] #['192.168.0.1', '10.0.0.1', '172.16.0.1', '192.168.0.2']\n",
        "\n",
        "def hash_ip(ip):\n",
        "    return hash(ip) % (10**8)  # Modulo to limit to a certain range of values\n",
        "\n",
        "hashed_ips = [hash_ip(ip) for ip in ip_addresses]\n",
        "print(hashed_ips)\n",
        "X['hashed_ips'] = hashed_ips\n",
        "\n",
        "\n",
        "# embedding IP Address column due to its large number of unique values\n",
        "# !pip install gensim\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# ip_addresses = X['IP Address']\n",
        "\n",
        "# # Tokenize IP addresses\n",
        "# tokenized_ips = [ip.split('.') for ip in ip_addresses]\n",
        "\n",
        "# # Train Word2Vec model\n",
        "# model = Word2Vec(sentences=tokenized_ips, vector_size=4, window=1, min_count=1, sg=0)\n",
        "\n",
        "# # Get embeddings for IP addresses\n",
        "# embedded_IP = [model.wv[ip] for ip in tokenized_ips]\n",
        "# X['embedded_IP'] = [np.mean(embedding) for embedding in embedded_IP]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.drop('IP Address', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5CgIHHrQ0h4d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X['hashed_ips'].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-zSPJhHuvr3",
        "outputId": "1ee9406e-5926-4c6f-bda2-0871318c6d74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i6-jm_YWHJGk"
      },
      "outputs": [],
      "source": [
        "X['Transaction Date and Time'] = pd.to_datetime(X['Transaction Date and Time'])\n",
        "X['Day of the Week'] = X['Transaction Date and Time'].dt.dayofweek\n",
        "X['Hour of the Day'] = X['Transaction Date and Time'].dt.hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dy9Sd4LAMUW",
        "outputId": "b999d4a0-3b41-4069-9ca2-2cad715faab1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Payment Method                       40\n",
              "Country Code                         40\n",
              "Transaction Type                     38\n",
              "Device Type                          38\n",
              "Browser Type                         39\n",
              "Operating System                     40\n",
              "Merchant Category                    40\n",
              "User Occupation                      26\n",
              "User Gender                           7\n",
              "User Account Status                  18\n",
              "Transaction Status                   40\n",
              "Transaction Time of Day               3\n",
              "User's Device Location               40\n",
              "Transaction Currency                 40\n",
              "Transaction Purpose                  38\n",
              "User's Email Domain                  40\n",
              "Transaction Authentication Method    39\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Converting obeject to category\n",
        "cat_col = X.dtypes[X.dtypes == 'object'].index.tolist()\n",
        "for col in cat_col:\n",
        "  X[col] = X[col].astype('category')\n",
        "cat_col = X.dtypes[X.dtypes == 'category'].index.tolist()\n",
        "X[cat_col].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[cat_col].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5L_aQWFy3_8",
        "outputId": "707ab4b6-1910-430a-a89e-43907a5c561e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Payment Method                       category\n",
              "Country Code                         category\n",
              "Transaction Type                     category\n",
              "Device Type                          category\n",
              "Browser Type                         category\n",
              "Operating System                     category\n",
              "Merchant Category                    category\n",
              "User Occupation                      category\n",
              "User Gender                          category\n",
              "User Account Status                  category\n",
              "Transaction Status                   category\n",
              "Transaction Time of Day              category\n",
              "User's Device Location               category\n",
              "Transaction Currency                 category\n",
              "Transaction Purpose                  category\n",
              "User's Email Domain                  category\n",
              "Transaction Authentication Method    category\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eioUsvsHDoPM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving preprocessed dataset. this is to conserve memory usage\n",
        "#X.to_csv('gdrive/MyDrive/edited_Fraud_Detection_Dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "Vtj-ahMTnZFb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofr0kR-AoaXH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZOCdq0gqo0J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LJ84yxTWD-VT"
      },
      "outputs": [],
      "source": [
        "# X = pd.read_csv('gdrive/MyDrive/edited_Fraud_Detection_Dataset.csv')\n",
        "# cat_col = X.dtypes[X.dtypes == 'categoryt'].index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3reIc6cDXFX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz8nARFefw6j"
      },
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIxOyfFDId_g"
      },
      "outputs": [],
      "source": [
        "# for variables with large number of unique values, frequency encoding seems to be the best shot\n",
        "\n",
        "for column in ['Payment Method', 'Country Code', 'Transaction Type', 'Transaction Authentication Method', 'Device Type', 'Browser Type', 'Operating System', 'Merchant Category', 'Transaction Status', 'User\\'s Device Location', 'Transaction Currency', 'Transaction Purpose', 'User\\'s Email Domain']:\n",
        "    frequency_counts = X[column].value_counts(normalize=True)\n",
        "    X[column + '_Frequency'] = X[column].map(frequency_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH4g_y6YJxHm"
      },
      "outputs": [],
      "source": [
        "# Remove moderate number of unique values\n",
        "\n",
        "# X_encoded = pd.get_dummies(X, columns=['User Occupation', 'User Account Status'])\n",
        "X_encoded = pd.get_dummies(X, columns=['Transaction Time of Day'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# List of features to label encode\n",
        "features_to_label_encode = ['User Occupation', 'User Gender', 'User Account Status']\n",
        "\n",
        "# Apply label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "for feature in features_to_label_encode:\n",
        "    X_encoded[feature + '_Encoded'] = label_encoder.fit_transform(X_encoded[feature])\n"
      ],
      "metadata": {
        "id": "nL5iSqvIjXbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S21hakqBMHOr"
      },
      "outputs": [],
      "source": [
        "X_encoded.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VMEc1m8M3yO"
      },
      "outputs": [],
      "source": [
        "# Removing Original Columns\n",
        "# List of original categorical columns to remove\n",
        "categorical_columns_to_remove = ['User Occupation', 'User Gender', 'User Account Status','Payment Method', 'Country Code', 'Transaction Type', 'Transaction Authentication Method', 'Device Type', 'Browser Type', 'Operating System', 'Merchant Category', 'Transaction Status', 'User\\'s Device Location', 'Transaction Currency', 'Transaction Purpose', 'User\\'s Email Domain']\n",
        "\n",
        "# Drop the original categorical columns\n",
        "X_encoded = X_encoded.drop(columns=categorical_columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koIlrqncNSib"
      },
      "outputs": [],
      "source": [
        "cat_obj = X_encoded.dtypes[X_encoded.dtypes=='category'].index.tolist()\n",
        "for col in cat_obj:\n",
        "  X_encoded[col] = X_encoded[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded.dtypes\n"
      ],
      "metadata": {
        "id": "9v00Wyxl_hcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq76Ruz7Nu41"
      },
      "outputs": [],
      "source": [
        "X_encoded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWJrIUaqN5Sx"
      },
      "outputs": [],
      "source": [
        "X_encoded.drop('Transaction Date and Time', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI2DlmiyOLN5"
      },
      "outputs": [],
      "source": [
        "X_encoded.dtypes==\"category\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Y3v0b7IJdst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf_-0it1jN5M"
      },
      "outputs": [],
      "source": [
        "small_data = X_encoded.sample(n=1000000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVEldTH4Pyn0"
      },
      "outputs": [],
      "source": [
        "X_data = small_data.drop('Fraudulent Flag', axis = 1)\n",
        "y_data = small_data['Fraudulent Flag']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qWIu4r4Q99N"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZncVsWIRCA9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=20)\n",
        "X_selected = selector.fit_transform(X_data, y_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JohkIJvtRLnE"
      },
      "outputs": [],
      "source": [
        "# Get the indices of the selected features\n",
        "selected_indices = selector.get_support(indices=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH_VVN8oR3DA"
      },
      "outputs": [],
      "source": [
        "# Get the corresponding feature names\n",
        "selected_features = X_data.columns[selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PAgr9PgR4yb"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with selected features\n",
        "selected_features_df = X_data[selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CnG7v0IqalC"
      },
      "outputs": [],
      "source": [
        "# Create a visualization (assuming X_selected is a DataFrame)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming X_selected is a DataFrame\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(selected_features, selector.scores_[selected_indices])\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Mutual Information Score')\n",
        "plt.title('Top 10 Selected Features')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# view the selected features in a DataFrame\n",
        "print(selected_features_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1B9xmKoOPUe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG1ITLm1szIo"
      },
      "outputs": [],
      "source": [
        "# Splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features_df, y_data, test_size=0.2, stratify=y_data, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Enable GPU in Google Colab\n",
        "# Go to \"Runtime\" -> \"Change runtime type\" -> Select \"GPU\" -> Click \"Save\"\n",
        "\n",
        "# Step 2: Mount Google Drive (Optional, for saving models)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# Step 3: Install Required Libraries (if not already installed)\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "torch.cuda.set_per_process_memory_fraction(0.5)  # Limit to 50% of GPU memory\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class FraudDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(FraudDetectionModel, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Assuming 'final_df' is your preprocessed DataFrame with features and target variable\n",
        "# X = final_df.drop(columns=['Fraudulent Flag']).values\n",
        "# y = final_df['Fraudulent Flag'].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(selected_features_df)\n",
        "\n",
        "#X_train, X_valid, y_train, y_valid = train_test_split(X, y_data, test_size=0.2, random_state=42)\n",
        "# X_test = X_test.values\n",
        "# y_test = y_test.values\n",
        "# Convert data to PyTorch tensors and move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = torch.Tensor(X_train).to(device)\n",
        "y_train = torch.Tensor(y_train).to(device).squeeze()\n",
        "X_valid = torch.Tensor(X_test).to(device)\n",
        "y_valid = torch.Tensor(y_test).to(device).squeeze()\n",
        "\n",
        "# Define the model and move it to GPU\n",
        "input_size = X_train.shape[1]\n",
        "model = FraudDetectionModel(input_size).to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Hyperparameters for fine-tuning\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "# Learning Rate Schedule (adjust as needed)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Early Stopping (adjust patience as needed)\n",
        "best_valid_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        inputs = X_train[i:i+batch_size]\n",
        "        labels = y_train[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_outputs = model(X_valid).squeeze()\n",
        "        valid_loss = criterion(valid_outputs, y_valid)\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Early stopping\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            counter = 0\n",
        "            # Optional: Save the best model\n",
        "            torch.save(model.state_dict(), '/content/gdrive/My Drive/best_fraud_detection_model.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "# Optional: Load the best model\n",
        "model.load_state_dict(torch.load('/content/gdrive/My Drive/best_fraud_detection_model.pth'))\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    valid_outputs = model(X_valid)\n",
        "    valid_preds = (valid_outputs > 0.5).float()\n",
        "\n",
        "    accuracy = (valid_preds == y_valid).sum().item() / len(y_valid)\n",
        "    print(f'Validation Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "MkkWZJgQT9Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZsr5yh7qhzy"
      },
      "outputs": [],
      "source": [
        "# # build the model\n",
        "\n",
        "# # from imblearn.over_sampling import SMOTE\n",
        "# # X_resampled, y_resampled = SMOTE().fit_resample(X_train_encoded, y_train_small)\n",
        "# # X_test_resampled, y_test_resampled = SMOTE().fit_resample(X_test_encoded, y_test_small)\n",
        "\n",
        "# # X_resampled.drop(['Year', 'Month', 'Day'], axis=1, inplace=True)\n",
        "# # X_test_resampled.drop(['Year', 'Month', 'Day'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "dtrain = xgb.DMatrix(data=X_train, label= y_train, enable_categorical=True)\n",
        "dtest = xgb.DMatrix(data=X_test, label= y_test, enable_categorical=True)\n",
        "\n",
        "# # # Assuming 'X_train' and 'y_train' are your training data\n",
        "\n",
        "# # # Initialize the XGBoost model with categorical support enabled\n",
        "#model = xgb.XGBClassifier()\n",
        "\n",
        "# # # Define the parameter grid for hyperparameter tuning\n",
        "# # param_grid = {\n",
        "# #     'max_depth': [3, 7],\n",
        "# #     'min_child_weight': [1, 5],\n",
        "# #     'subsample': [0.8, 1.0],\n",
        "# #     'colsample_bytree': [0.8, 1.0],\n",
        "# #     'learning_rate': [0.1, 0.01, 0.5, 0.9],\n",
        "# #     'n_estimator': [200]\n",
        "# # }\n",
        "\n",
        "# scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "\n",
        "params = {\n",
        "    'min_child_weight': 7,\n",
        "    'subsample': 0.577,\n",
        "    'max_depth': 10,\n",
        "    'reg_lambda': 0.377,\n",
        "    'learning_rate': 0.017,\n",
        "    'colsample_bytree': 0.974,\n",
        "    'reg_alpha': 0.206,\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'logloss',\n",
        "    'n_estimator': [200]\n",
        "\n",
        "}\n",
        "# # # Initialize GridSearchCV\n",
        "# # grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=3, verbose=2, n_jobs=-1,  error_score='raise')\n",
        "\n",
        "# # # Perform the hyperparameter tuning\n",
        "# # grid_search.fit(X_train_encoded, y_train_small)\n",
        "\n",
        "# # # Get the best hyperparameters\n",
        "# # best_params = grid_search.best_params_\n",
        "\n",
        "# # # Train the model with the best hyperparameters\n",
        "best_model = xgb.XGBClassifier(params, verbosity=2)\n",
        "best_model =xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# # # Evaluate the model\n",
        "# y_pred = best_model.predict(dtest)\n",
        "# # accuracy = accuracy_score(y_test_resampled, y_pred)\n",
        "\n",
        "# # # Display results\n",
        "# # #print(f'Best Hyperparameters: {best_params}')\n",
        "# # print(f'Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPtEUjKQtoAt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = best_model.predict(dtest)\n",
        "\n",
        "#Convert predicted probabilities to binary predictions (0 or 1)\n",
        "y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]\n",
        "\n",
        "#Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "report = classification_report(y_test, y_pred_binary)\n",
        "\n",
        "#Display results\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cNE_j9Sv5Cf"
      },
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_standardized = scaler.fit_transform(X_train)\n",
        "# X_test_standardized = scaler.fit(X_test)\n",
        "\n",
        "# params = {\n",
        "#     'objective': 'binary:logistic',\n",
        "#     'min_child_weight': 7,\n",
        "#     'subsample': 0.577,\n",
        "#     'max_depth': 10,\n",
        "#     'reg_lambda': 0.377,\n",
        "#     'learning_rate': 0.017,\n",
        "#     'colsample_bytree': 0.974,\n",
        "#     'reg_alpha': 0.206,\n",
        "#     'eval_metric': 'logloss',\n",
        "#     'scale_pos_weight': scale_pos_weight\n",
        "\n",
        "# }\n",
        "\n",
        "# # Initialize base models\n",
        "# xgb_model = xgb.XGBClassifier(params)\n",
        "# rf_model = RandomForestClassifier(n_estimators=1500, random_state=42)\n",
        "# # lr_model = LogisticRegression()\n",
        "\n",
        "# # Create a voting ensemble\n",
        "# ensemble_model = VotingClassifier(estimators=[\n",
        "#     ('xgb', xgb_model),\n",
        "#     ('rf', rf_model)\n",
        "#     #('lr', lr_model)\n",
        "# ], voting='soft')\n",
        "\n",
        "# # Train the ensemble model\n",
        "# ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# # Evaluate on the test set\n",
        "# y_pred = ensemble_model.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmjDPN60IDn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-WeukFJgq9l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEVCisbB0Lvs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# For Bayesian hyper-parameter optimization\n",
        "import hyperopt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import make_scorer, precision_recall_curve, recall_score, precision_score\n",
        "from functools import partial\n",
        "# To supress a deprecation warning caused due to an issue between XGBoost and SciPy\n",
        "import warnings\n",
        "\n",
        "MIN_PRECISION = 0.05\n",
        "\n",
        "# The current version of XGBoost uses a conditional statement that\n",
        "# the current version SciPy (internally used by XGBoost) doesn't like.\n",
        "# This supresses SciPy's deprecation warning message\n",
        "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwZPAwqr9F3v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNju720o34JO"
      },
      "source": [
        "\n",
        "**Defining the Objective Function for Hyperopt**\n",
        "\n",
        "To enable Hyperopt to optimize the model, it's essential to establish the objective function for optimization. In this context, the objective function is essentially the cross-validated score of the model, based on the specified hyperparameter combination we aim to assess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1RkZ2hi9MOz"
      },
      "outputs": [],
      "source": [
        "def objective(params, X, y, X_early_stop, y_early_stop, scorer, n_folds = 10):\n",
        "\n",
        "    pos_count = y_train.sum()\n",
        "    neg_count = len(y_train) - pos_count\n",
        "    imbalance_ratio = neg_count / pos_count\n",
        "\n",
        "    xgb_clf = XGBClassifier(**params, scale_pos_weight=imbalance_ratio,\n",
        "                            n_estimators = 2000, n_jobs = 1)\n",
        "\n",
        "    xgb_fit_params = {'early_stopping_rounds': 50,\n",
        "                      'eval_metric': ['logloss'],\n",
        "                      'eval_set': [(X_early_stop, y_early_stop)],\n",
        "                      'verbose': False\n",
        "                      }\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(xgb_clf, X_train, y_train, cv = n_folds,\n",
        "                               fit_params = xgb_fit_params, n_jobs = -1,\n",
        "                               scoring = scorer))\n",
        "\n",
        "    # hypoeropt minimizes the loss, hence the minus sign behind cv_score\n",
        "    return {'loss': -cv_score, 'status': hyperopt.STATUS_OK, 'params': params}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRHYQ3z89NTF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdMLzmeG4OZ_"
      },
      "source": [
        "**Evaluation Metric**\n",
        "\n",
        "The Conditional Recall metric, which will be incorporated using the scorer argument in the objective function, is outlined as follows. It essentially extracts the recall rate corresponding to the provided precision level from the precision-recall curve. The default precision level (utilized for all reported results here) is fixed at 0.05, but it can be adjusted as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovEbmqpg9NzQ"
      },
      "outputs": [],
      "source": [
        "def conditional_recall_score(y_true, pred_proba, precision = MIN_PRECISION):\n",
        "    # Since the PR curve is discreet it might not contain the exact precision value given\n",
        "    # So we first find the closest existing precision to the given level\n",
        "    # Then return the highest recall acheiveable at that precision level\n",
        "    # Taking max() helps in case PR curve is locally flat\n",
        "    # with multiple recall values for the same precision\n",
        "    pr, rc,_ = precision_recall_curve(y_true, pred_proba[:,1])\n",
        "    return np.max(rc[pr >= min_prec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88fIUXTA9ROb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRRjAr6p4a1E"
      },
      "source": [
        "**Optimizing the Cross Validated Score**\n",
        "\n",
        "Next, we'll integrate these components and utilize Hyperopt's optimization function, fmin, to explore the most favorable parameter combination. I'll create a new function that executes this optimization and provides the parameters that yield the highest cross-validated score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRMFCHaj9Smr"
      },
      "outputs": [],
      "source": [
        "def tune_xgb(param_space, X_train, y_train, X_early_stop, y_early_stop, n_iter):\n",
        "    scorer = make_scorer(conditional_recall_score, needs_proba=True)\n",
        "\n",
        "    # hyperopt.fmin will only pass the parameter values to objective. So we need to\n",
        "    # create a partial function to bind the rest of the arguments we want to pass to objective\n",
        "    obj = partial(objective, scorer = scorer, X = X_train, y = y_train,\n",
        "                  X_early_stop = X_early_stop, y_early_stop = y_early_stop)\n",
        "\n",
        "    # A trials object that will store the results of all iterations\n",
        "    trials = hyperopt.Trials()\n",
        "\n",
        "    hyperopt.fmin(fn = obj, space = param_space, algo = hyperopt.tpe.suggest,\n",
        "                         max_evals = n_iter, trials = trials)\n",
        "\n",
        "    # returns the values of parameters from the best trial\n",
        "    return trials.best_trial['result']['params']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2DpexXz9VrD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMPjPFvF4qsa"
      },
      "source": [
        "**Parameter Space Specification**\n",
        "\n",
        "As previously mentioned, Bayesian optimization relies on assuming an initial probability distribution for parameters and refining this distribution with each iteration. Therefore, instead of providing a list of values as in a grid search, we need to specify the prior distributions of parameters. This can be achieved using Hyperopt's built-in probability distribution functions. The dictionary object below outlines the parameter space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwBLN6i89WWE"
      },
      "outputs": [],
      "source": [
        "param_space = {\n",
        "        'learning_rate': hyperopt.hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
        "        'max_depth': hyperopt.hp.choice('max_depth', [2, 4, 6, 8, 10]),\n",
        "        'subsample': hyperopt.hp.uniform('subsample', 0.25, 1),\n",
        "        'colsample_bytree': hyperopt.hp.uniform('colsample_bytree', 0.7, 1.0),\n",
        "        'min_child_weight': hyperopt.hp.choice('min_child_weight', [1, 3, 5, 7]),\n",
        "        'reg_alpha': hyperopt.hp.uniform('reg_alhpa', 0, 1.0),\n",
        "        # Avoiding lambda = 0. There is a Github issue on strange behaviour with lambda = 0\n",
        "        'reg_lambda': hyperopt.hp.uniform('reg_lambda', 0.01, 1.0),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oD8Kzfc9aZG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIwKPqU59azV"
      },
      "outputs": [],
      "source": [
        "def optimal_threshold(estimator, X, y, n_folds = 10, min_prec = 0.05, fit_params = None):\n",
        "\n",
        "    cv_pred_prob = cross_val_predict(estimator, X, y, method='predict_proba',\n",
        "                                     cv = n_folds, fit_params=fit_params, n_jobs=-1)[:,1]\n",
        "\n",
        "    # Once again, the PR curve is discreet and may not contain the exact precision level\n",
        "    # we are looking for. So, we need to find the closest existing precision\n",
        "    pr, _, threshold = precision_recall_curve(y, cv_pred_prob)\n",
        "    # precision is always one element longer than threshold and the last one is always set to 1\n",
        "    # So I drop the last element of precision so I can use it below to index threshold\n",
        "    pr = pr[:-1]\n",
        "    return min(threshold[pr >= min_prec])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OeHV8YA9ewJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg5JRHiv9ffX"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Loading the data\n",
        "    # data = pd.read_csv('creditcard.csv')\n",
        "    # X = data.drop('Class', axis = 1)\n",
        "    # y = data['Class']\n",
        "\n",
        "    # # Train/test split, 80/20, random_state set for reproducibility\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,\n",
        "    #                                                     test_size = 0.2, random_state = 1)\n",
        "\n",
        "    # Further splitting the initial training set so that 10% of all data(1/8 of 80%)\n",
        "    # can be used as the evaluation set by XGBoost for early stopping\n",
        "    X_train, X_early_stop, y_train, y_early_stop = train_test_split(X_train, y_train,test_size = 1/8,\n",
        "                                                                    stratify = y_train, random_state = 1)\n",
        "\n",
        "    # The prior probability distribution of parameters for Bayesian optimization\n",
        "    param_space = {\n",
        "            'learning_rate': hyperopt.hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
        "            'max_depth': hyperopt.hp.choice('max_depth', [2, 4, 6, 8, 10]),\n",
        "            'subsample': hyperopt.hp.uniform('subsample', 0.25, 1),\n",
        "            'colsample_bytree': hyperopt.hp.uniform('colsample_bytree', 0.7, 1.0),\n",
        "            'min_child_weight': hyperopt.hp.choice('min_child_weight', [1, 3, 5, 7]),\n",
        "            'reg_alpha': hyperopt.hp.uniform('reg_alhpa', 0, 1.0),\n",
        "            # Avoiding lambda = 0. There is a Github issue on strange behaviour with lambda = 0\n",
        "            'reg_lambda': hyperopt.hp.uniform('reg_lambda', 0.01, 1.0),\n",
        "            }\n",
        "\n",
        "    # # # # # # # # #\n",
        "    # Step 1: Tuning hyper-parameters of the XGBoost classifier\n",
        "    # # # # # # # # #\n",
        "    print('Step 1: Tuning hyper-parameters using Bayesian Optimization\\n')\n",
        "\n",
        "    best_params = tune_xgb(param_space, X_train, y_train, X_early_stop, y_early_stop, n_iter = 150)\n",
        "\n",
        "    print('\\tThe best hyper-parameters found:\\n')\n",
        "    print(*['\\t\\t%s = %s' % (k, str(round(v, 4))) for k, v in best_params.items()], sep='\\n')\n",
        "\n",
        "    # # # # # # # # #\n",
        "    # Step 2: Empirical thresholding: finding optimal classification threshold\n",
        "    # # # # # # # # #\n",
        "    print('\\nStep 2: Empirical Thresholding\\n')\n",
        "\n",
        "    # I use 1500 trees which is very close to optimal n_trees found by early stopping while tuning\n",
        "    xgboost_clf = XGBClassifier(**best_params, n_estimators=1500)\n",
        "\n",
        "    classification_cutoff = optimal_threshold(xgboost_clf, X_train, y_train, min_prec = MIN_PRECISION)\n",
        "\n",
        "    print('\\tOptimal classification threshold = %1.3f' % classification_cutoff)\n",
        "\n",
        "    # # # # # # # # #\n",
        "    # Setp 3: Training and testing the model\n",
        "    # # # # # # # # #\n",
        "    print('\\nStep 3: Training and testing the model\\n')\n",
        "\n",
        "    # Training on all the training data (excluding the small validation set to avoid overfitting)\n",
        "    xgboost_clf.fit(X_train, y_train, verbose = False)\n",
        "\n",
        "    y_pred = thresholded_predict(X_test, xgboost_clf, threshold = classification_cutoff)\n",
        "\n",
        "    test_recall = recall_score(y_test, y_pred)\n",
        "    test_precision = precision_score(y_test, y_pred)\n",
        "\n",
        "    print('\\tTest set performance:')\n",
        "    print('\\tRecall    = %2.3f' % test_recall)\n",
        "    print('\\tPrecision = %2.3f' % test_precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh8ULpiN9jH3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0pfSg1J9jmz"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import precision_recall_curve, auc, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "logistic_clf = Pipeline(steps = [('Scaler', StandardScaler()),\n",
        "                                 ('Sampler', SMOTE()),\n",
        "                                 ('Logistic', LogisticRegression())])\n",
        "\n",
        "param_grid = {'Logistic__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "scorer = make_scorer(recall_at_precision, needs_proba=True)\n",
        "\n",
        "grid_search = GridSearchCV(logistic_clf, param_grid, cv = 10,\n",
        "                           scoring = scorer, refit = True, n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train_all, y_train_all)\n",
        "\n",
        "# Because of refit = True in GridSearchCV the best estimator has already been fitted to training data\n",
        "scores = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
        "pr, rc, thrs = precision_recall_curve(y_test, scores)\n",
        "pr_auc = auc(rc, pr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLANZRxG9pM7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybfGV8k09pqC"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, auc, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "forest_clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1)\n",
        "\n",
        "param_grid = {'max_features': [None, 'sqrt', 15],\n",
        "              'min_samples_leaf': [1, 20, 50, 100],\n",
        "              'max_depth': [4, 10, 20, None]}\n",
        "\n",
        "grid_search = GridSearchCV(forest_clf, param_grid, cv = 10,\n",
        "                           scoring = scorer, refit=True)\n",
        "\n",
        "grid_search.fit(X_train_all, y_train_all)\n",
        "\n",
        "scores = grid_search.best_estimator_.predict_proba(X_test)[:,1]\n",
        "pr, rc, thrs = precision_recall_curve(y_test, scores)\n",
        "pr_auc = auc(rc, pr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuaqcudF9stw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BigFTnW59tEd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOlWVWUzOQsvs+F3AGEYn11",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}